\chapter{Problem Definition}
\label{prob-def}
\section{Introduction}
Cloud computing has been on rise over the last decade. Parts of this popularity is due to its inherent features. It lets application developers to run their applications on virtual infrastructure. Virtual infrastructure lays the foundation of on-demand infrastructure. Developers acquire and release resources as required by workload. Examples of cloud providers are Amazon AWS~\cite{aws}, Microsoft Azure~\cite{azure} and Google Cloud~\cite{gc}. Today's cloud infrastructure is widely used by many customers for different purposes such as batch processing, serving static content, storage servers and alike.

As cloud environment brings up \emph{elasticity}~\cite{Roman:2013}, it also introduces a new set of challenges and problems. Modern applications face fluctuating workloads. Typically, if a workload is \emph{predictable}, resources are allocated ahead of time before load-spike starts. However, in many other scenarios predicting even near future workload is a not so easy task. Even though running an application in cloud environments helps to overcome a long standing problem of \emph{over-provisioning}, low utilization is still one of the major problems of cloud applications. This has been confirmed by multiple studies~\cite{Delimitrou:2014}~\cite{Reiss:2012}.

The root of the problem is originated from the fact that, most developers do not have enough insight about bottom and peak workload of their application. Thus, they fail to define an effective scaling strategy. Therefore, they end up with conservative strategies which in turn leads to low utilization. Hence, we need a system that automates the process of resource allocation. Auto-Scaling has been well studied in the context of web application.~\cite{Hasan2012IntegratedAA}~\cite{Dutreilh2010}~\cite{Herbst:2013} are just a few examples. Chapter~\ref{related} explores more techniques and strategies.

\section{Objectives of Auto-Scaling Systems}
The ultimate goal of an Auto-Scaling system is to automate the process of acquiring and releasing \emph{resources} in order to minimize the \emph{cost} with minimum violation of \emph{service level objectives} (SLO). The definition of \emph{resource} depends on the context. As an example, for a stateless web applications it means virtual machines or containers that run web server software. For an Auto-Scaling system to adjust required resources, it shall consider different aspects of the application and its environment. Additionally, the term \emph{cost} is also defined in the context. As an example, it might mean monetary cost or just numerical value of resources. SLOs are predefined rules that shall not be violated during application runtime. These rules are also defined in the context of application.

\section{Auto-Scaling in Data Stream Processing Systems}
\emph{Data Stream Processing Systems} are data processing systems that process \emph{unbounded} stream of data unlike their \emph{batch-oriented} counterparts. With the ever increasing adoption of IoT applications, it is critical to design stream processing systems that handles the incoming messages with high throughput and low latency. With static workloads, these problems could be solved by dominating stream processing systems like Apache Spark~\cite{spark}, Apache Storm~\cite{Storm} and Apache Flink~\cite{flink}. However, the problem of low utilization still holds in data stream processing systems. This leads us to a new generation of stream processing systems called \emph{Elastic Data Stream Processing Systems} that applies elasticity concepts to stream processing system.

Prior to this thesis a number of studies have been performed on elastic stream processing system. \cite{CastroFernandez:2013},~\cite{Heinze:2014} are just a few samples. One of the dominating stream processing systems is Apache Spark which supports both batch and stream processing. In order to support both workloads, Apache Spark has a unique architecture that partitions the input workload into predefined window of batches -- an architecture known as micro batching. With this common architecture as a foundation, number of interesting challenges arise that need to be considered for elastic workloads.

This thesis will focus on dynamic resource allocation in the context of Apache Spark. An extensible framework will be developed based on a prior work by \textcite{Michal:2017}. This thesis will extend the existing prototype and implement multiple Auto-Scaling techniques for Apache Spark Streaming and will evaluate these techniques using real-world workloads. The ultimate goal is to identify how the architecture of Spark Streaming influences the performance of the different Auto-Scaling techniques.

\section{Requirements of Thesis}
Since Auto-Scaling is a quite wide realm, this thesis focuses on Apache Spark Streaming with a couple of predefined requirements. Any proposed solution must adhere to the following requirements and limitations to a large extent.
\begin{itemize}
    \item \textbf{Online Decision Making}. Since Spark Streaming is an online data stream processing system, any proposed solution shall apply scaling decisions in an online manner without causing any downtime on target system.
    \item \textbf{QoS Guarantees}. Scaling decisions shall avoid violating predefined SLOs as much as possible. Understandably, violating SLOs is an inevitable incident. However, this should be kept under an acceptable level.
    \item \textbf{Extendibility}. Any proposed solution shall provide some degree of extendibility such that, future modifications can be applied without any major code refactoring.
    \item \textbf{Reconfigurability}. Any proposed solution should be easily configurable by administration or \emph{devops} team.
    \item \textbf{Workload Independence}. Any proposed solution should not impose any assumption on type of workload running under Apache Spark Streaming.
    \item \textbf{Computationally Feasible}. As mentioned in first requirement, any proposed solution should be computationally feasible such that it could generate results in order of seconds.
    \item \textbf{Without Spark Core Modification}. In order to make the solution extensible as much as possible, any proposed solution, is not allowed to modify \lstinline$spark-core$ or \lstinline$spark-streaming$ packages.
\end{itemize}

\section{Summary}
As mentioned this thesis will focus on dynamic resource allocation in the context of Apache Spark. The thesis is organized as follows. Chapter~\ref{intro-auto-scale} introduces and explains basics of Auto-Scaling techniques. Chapter~\ref{spark} introduces the architecture of Apache Spark Streaming. Chapter~\ref{design} explains structure and design considerations of this thesis including implementation details. Chapter~\ref{eval} evaluates the implementation under different workloads using different set of configuration parameters. Chapter~\ref{related} includes discussion of prior and related work. Finally, chapter~\ref{conc} concludes.