\chapter{Related Work}
Dynamic resource scaling in cloud environments has been studied extensively in literature. As a naive implementation, there are two thresholds, namely \emph{upper bound} and \emph{lower bound}. However, such an implementation suffers from \emph{oscillating} decisions. In order to remedy this issue, \emph{grace period} shall be enforced. During this period, no scaling decision is made. \newline

\textcite{Hasan2012IntegratedAA} introduced four thresholds and two time periods. \lstinline$ThrUpper$ defines upper bound. \lstinline$ThrBelowUpper$ is slightly below \lstinline$ThrUpper$. Similarly, \lstinline$ThrLower$ defines lower bound and \lstinline$ThrAboveLower$ is slightly above the lower bound. In case, system utilization stays between \lstinline$ThrUpper$ and \lstinline$ThrBelowUpper$ for a specific duration, then cluster controller decides to take a scale-out action, by adding resources. On the other hand, if system utilization stays between \lstinline$ThrLower$ and \lstinline$ThrAboveLower$ for a specified duration, then controller decides to take scale-in action. Defining two levels of thresholds helps to detect workload \emph{persistency} and avoid making immature scaling decision. However, defining thresholds is a tricky and manual process, and need to be carefully done\cite{Dutreilh2010From}. It should be noted that, computation overhead of this approach is very low. \newline

\emph{RightScale} applies voting algorithm\cite{RightScale} among nodes to make scaling decision. In order for a specific action to be decided, majority of nodes should vote in favour of that action. Otherwise, no action is selected as a default action. Afterwards, nodes apply grace period to stabilize the cluster. The complexity of the voting process in trusted environments is in the order of $O(n^2)$, which leads to heavy network traffic among participants when cluster size grows. This approach is also categorized in threshold based approaches. Thus, it suffers from the same issue as mentioned above.\newline

\textcite{Herbst:2013} surveys different auto-scaling techniques based on time-series analysis in order to forecast \emph{trends} and \emph{seasons}. \emph{Moving Average Method} takes the average over a sliding window and smooths out minor noise level. Its computational overhead is proportional to size of the window. \emph{Simple Exponential Smoothing}(SES) goes further than just taking average. It gives more weight to more recent values in sliding window by an exponential factor. Although it is more computationally intensive compared to moving average, it is still negligible. SES is capable of detecting short-term trends but fails at predicting seasons. These approaches are more specific instances of \emph{ARIMA} (Auto-Regressive Integrated Moving Average) which is a general purpose framework to calculate moving averages. However, time-series analysis is only suitable for stationary problems consist of recurring workload patterns such as web applications. In case of streaming (specially in multi-tenant environments), in which the workload is highly unpredictable, time-series analysis tends to be less useful. Additionally, more advanced forms of time-series analysis which are capable of forecasting seasons (such as \emph{tBATS Innovation State Space Modelling Framework}\cite{Alysha2011}, \emph{ARIMA Stochastic Process Modelling Framework}\cite{JSSv027i03}) are computationally infeasible for streaming workloads. \newline

\textcite{Herbst:2017} continues the survey on state of the art techniques to predict future workload. It includes workload forecasting based on \emph{Bayesian Networks}(BN) and \emph{Neural Networks}. There are issue with each of them that makes them unsuitable for streaming workloads. As an example, there is no universally applicable method to construct a BN. Furthermore, it requires collecting data and training the model offline. Neural networks suffer from the same issue. That is, it requires collecting samples and training the model offline. For complex models, training phase is typically computationally infeasible which is conflicting with requirements of thesis.
\newpage
\textcite{Tesauro2006} proposes a hybrid approach to overcome poor performance of online training. The system consists of two components: an online component based on queuing system combined with reinforcement learning component that is trained offline. The offline component is based on \emph{neural networks}. Author models the data center as multiple applications managed under a single resource manager. Modelling streaming workloads as a queuing system has two problems. First, modelling is a complicated process and determining probability distributions requires domain knowledge. Second, it requires access to each node (so it can be modelled as a queue) which is currently not possible without modifying \lstinline$spark-core$ package. Since, it was one the requirements to provide a solution without making any modification to \lstinline$spark-core$, this work has been abandoned.\newline

\textcite{Rao:2009:VRL} proposed to use reinforcement learning to manage resources consumed by virtual machines. It employs standard model-free learning, which is known as \emph{temporal difference}\cite{rlIntro} or \emph{sarsa} algorithm. The state space consists of metrics collected from virtual machines (CPU, RAM, Network IO, \dots). There is no global controller and each node decides based on its own Q-Table. As mentioned in literature, standard temporal difference has a slow convergence speed. In order to speedup bootstrap phase, Q-Table is initialized by values that were obtained during separate supervised training. Since this approach also relies on offline training, it wasn't adopted by this thesis.\newline

\textcite{Heinze:2014} implemented reinforcement learning in the context of FUGU\cite{Grandl:2014:MPC} (which is a \emph{data stream processing} framework). Each node, has its own Q-Table and imposes local policy without coordinating to other nodes. This architecture can not be applied in context of spark streaming, since spark asbtracts away individual nodes from perspective of applicaiton developer. In order to decrease state space, the author applies two techniques. First, only system utilization is considered. Second, system utilization is discretized using coarse grained steps. To remedy slow convergence, controller enforces \emph{monotonicity constraint}\cite{Herodotou:2011}. That is, if controller decides to take scale-out action for a specific utilization, it may not decide scale-in for even worse system utilization. This feature has been adopted by this thesis.\newline

\textcite{Enda:2012} proposed a parallel architecture to reinforcement learning. Standard model-free learning (temporal difference) is used. No global controller is involved and each node decides locally. In order to speed up learning, all nodes, maintain two Q-Tables (local and global tables). Local table is learned and updated by each node. Whenever, an agent learns a new value for a specific state, it broadcasts it to other agents. Global table contains values received from other agents. Additionally, agent prioritize local and global tables by assigning weights to each table. Weights are factors that are defined by application developers. Final decision is the outcome of combining local and global tables. Although each node learns some part of the state space (which may overlap with other nodes), it is not applicable in the context of spark streaming. The assumption in this architecture is that, each node is operating autonomously without intervention of other nodes (such as web servers). In contrast, spark is a centrally managed system. That is, all nodes running spark jobs are supervised by a single master node (probably with couple of backup masters). \newline

