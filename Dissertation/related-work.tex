\chapter{Related Work}
Dynamic resource scaling in cloud environments has been studied extensively in literature. As a naive implementation, there are two thresholds, namely \emph{upper bound} and \emph{lower bound}. However, such an implementation suffers from \emph{oscillating} decisions. In order to remedy this issue, \emph{grace period} shall be enforced. During this period, no scaling decision is made. \newline

\textcite{Hasan2012IntegratedAA} introduced four thresholds and two time periods. \lstinline$ThrUpper$ defines upper bound. \lstinline$ThrBelowUpper$ is slightly below \lstinline$ThrUpper$. Similarly, \lstinline$ThrLower$ defines lower bound and \lstinline$ThrAboveLower$ is slightly above the lower bound. In case, system utilization stays between \lstinline$ThrUpper$ and \lstinline$ThrBelowUpper$ for a specific duration, then cluster controller decides to take a scale-out action, by adding resources. On the other hand, if system utilization stays between \lstinline$ThrLower$ and \lstinline$ThrAboveLower$ for a specified duration, then controller decides to take scale-in action. Defining two levels of thresholds helps to detect workload \emph{persistency} and avoid making immature scaling decision. However, defining thresholds is a tricky and manual process, and need to be carefully done\cite{Dutreilh2010}. It should be noted that, computation overhead of this approach is very low. \newline

\emph{RightScale} applies voting algorithm\cite{RightScale} among nodes to make scaling decision. In order for a specific action to be decided, majority of nodes should vote in favour of that action. Otherwise, no action is selected as a default action. Afterwards, nodes apply grace period to stabilize the cluster. The complexity of the voting process in trusted environments is in the order of $O(n^2)$, which leads to heavy network traffic among participants when cluster size grows. This approach is also categorized in threshold based approaches. Thus, it suffers from the same issue as mentioned above.\newline

\textcite{Herbst:2013} surveys different auto-scaling techniques based on time-series analysis in order to forecast \emph{trends} and \emph{seasons}. \emph{Moving Average Method} takes the average over a sliding window and smooths out minor noise level. Its computational overhead is proportional to size of the window. \emph{Simple Exponential Smoothing}(SES) goes further than just taking average. It gives more weight to more recent values in sliding window by an exponential factor. Although it is more computationally intensive compared to moving average, it is still negligible. SES is capable of detecting short-term trends but fails at predicting seasons. These approaches are more specific instances of \emph{ARIMA} (Auto-Regressive Integrated Moving Average) which is a general purpose framework to calculate moving averages. However, time-series analysis is only suitable for stationary problems consist of recurring workload patterns such as web applications. In case of streaming (specially in multi-tenant environments), in which the workload is highly unpredictable, time-series analysis tends to be less useful. Additionally, more advanced forms of time-series analysis which are capable of forecasting seasons (such as \emph{tBATS Innovation State Space Modelling Framework}\cite{Alysha2011}, \emph{ARIMA Stochastic Process Modelling Framework}\cite{JSSv027i03}) are computationally infeasible for streaming workloads. \newline

\textcite{Herbst:2017} continues the survey on state of the art techniques to predict future workload. It includes workload forecasting based on \emph{Bayesian Networks}(BN) and \emph{Neural Networks}. There are several issues with each of them that makes them unsuitable for streaming workloads. As an example, there is no universally applicable method to construct a BN. Furthermore, it requires collecting data and training the model offline. Neural networks suffer from the same issues. That is, it requires collecting samples and training the model offline. For complex models, training phase is typically computationally infeasible which is conflicting with requirements of thesis.
\newpage
\textcite{Tesauro2006} proposes a hybrid approach to overcome poor performance of online training. The system consists of two components: an online component based on queuing system combined with reinforcement learning component that is trained offline. The offline component is based on \emph{neural networks}. Author models the data center as multiple applications managed under a single resource manager. Modelling streaming workloads as a queuing system has two problems. First, modelling is a complicated process and determining probability distributions requires domain knowledge. Second, it requires access to each node (so it can be modelled as a queue) which is currently not possible without modifying \lstinline$spark-core$ package. Since, it was one the requirements to provide a solution without making any modification to \lstinline$spark-core$, this work has been abandoned.\newline

\textcite{Rao:2009:VRL} proposed to use reinforcement learning to manage resources consumed by virtual machines. It employs standard model-free learning, which is known as \emph{temporal difference}\cite{rlIntro} or \emph{sarsa} algorithm. The state space consists of metrics collected from virtual machines (CPU, RAM, Network IO, \dots). There is no global controller and each node decides based on its own Q-Table. As mentioned in literature, standard temporal difference has a slow convergence speed. In order to speedup bootstrap phase, Q-Table is initialized by values that were obtained during separate supervised training. Since this approach also relies on offline training, it wasn't adopted by this thesis.\newline

\textcite{Heinze:2014} implemented reinforcement learning in the context of FUGU\cite{Grandl:2014:MPC} (which is a \emph{data stream processing} framework). Each node, has its own Q-Table and imposes local policy without coordinating to other nodes. This architecture can not be applied in context of spark streaming, since spark asbtracts away individual nodes from perspective of applicaiton developer. In order to decrease state space, the author applies two techniques. First, only system utilization is considered. Second, system utilization is discretized using coarse grained steps. To remedy slow convergence, controller enforces \emph{monotonicity constraint}\cite{Herodotou:2011}. That is, if controller decides to take scale-out action for a specific utilization, it may not decide scale-in for even worse system utilization. This feature has been adopted by this thesis.\newline

\textcite{Enda:2012} proposed a parallel architecture to reinforcement learning. Standard model-free learning (temporal difference) is used. No global controller is involved and each node decides locally. In order to speed up learning, all nodes, maintain two Q-Tables (local and global tables). Local table is learned and updated by each node. Whenever, an agent learns a new value for a specific state, it broadcasts it to other agents. Global table contains values received from other agents. Additionally, agent prioritize local and global tables by assigning weights to each table. Weights are factors that are defined by application developers. Final decision is the outcome of combining local and global tables. Although each node learns some part of the state space (which may overlap with other nodes), it is not applicable in the context of spark streaming. The assumption in this architecture is that, each node is operating autonomously without intervention of other nodes (such as web servers). In contrast, spark is a centrally managed system. That is, all nodes running spark jobs are supervised by a single master node (probably with couple of backup masters). \newline

\textcite{CARDELLINI2018171} proposed a two level hierarchical architecture for resource management in Apache storm\cite{Storm}. There is a local controller on each node which is cooperating with a global controller. Local controller monitors each operator using different policies (threshold-based or reinforcement learning using temporal difference). In case, local controller decides to scale in or out an specific operator, it contacts the global controller and informs it about its decision. Then it waits to receive confirmation from global controller. Global controller operates using a token-bucket-based policy\cite{Valeria:2018} and has global view of cluster. It ranks requests coming from local controllers and either confirms or rejects their decisions. Although. this architecture seems to be a promising approach, however it has been implemented by modifying Storm's internal components. As mentioned above, this is in conflict with thesis's requirements.
\newpage
In order to mitigate the problem of large state space in reinforcement learning, \textcite{Lolos:2017} proposed to start the agent from small number of coarse grained states. As more metrics are collected (and stored as historical records), agent will discover \emph{outlier} parameters (those parameters that are affecting agent more, CPU rather than IO as an example). Then, it partitions the affected state into two states and \emph{re-trains} newly added states using historical records. Both temporal difference and value iteration methods can be used as learning algorithm. Gradually, agent only focuses on some specific parts of the state space, since all parameters are not equally important. This approach, effectively reduces the size of state space. However, the trade-off is the storage cost in which historical metrics need to be stored. It worth noting that from the context of paper, storage cost (whether it is in-memory or on-disk and the duration of storing historical metrics) is unclear. Thus, this approach has been abandoned due to uncertainty.\newline

\textcite{dutreilh:hal-01122123} proposed a model-based reinforcement learning approach for resource management of cloud applications. All virtual machines are supervised by a single global controller. Slow convergence is bottleneck of model-free learning, in contrast to model-based learning. However, environment dynamics are not available at time of modelling. Authors proposed to estimate these parameters as more metrics are collected and then switch to \emph{value iteration}\cite{rlIntro} algorithm instead of \emph{temporal difference}. In short, for each visit of (state, action, reward, state) quadruple as $(s,a,r,s')$, the following state variables will be stored:
\begin{align*}
CountStateAction(s,a)					&=	CountStateAction(s,a)+1\\
RewardStateAction(s,a)				&=	RewardStateAction(s,a)+r\\
CountStateTransition(s,a,s)]	&=	CountStateTransition(s,a,s')+1
\end{align*}
Periodically two variables, namely $\overline{T}$ and $\overline{R}$, are computed as follows:
\begin{align*}
\overline{T(s,a,s')}		&=	\frac{CountStateTransition(s,a,s')}{CountStateAction(s,a)}\\
\overline{R[s,a]}			&=	\frac{RewardStateAction(s,a)}{CountStateAction(s,a)}
\end{align*}
As more metrics are collected, $\overline{T}$ and $\overline{R}$ become more accurate and can be directly used in \emph{Bellman} equation. Until enough measurements get collected, a separate initial reward function is used which is essentially the original reward function but with penalty costs removed. Furthermore, In order to reduce the state space -- tuple of [request/sec, number of VMs, average response time) -- there exists a predefined upper and lower bound for state variables and average response time is measured at granularity of seconds. This approach has been partially adopted by this thesis.\newline

\textcite{Lohrmann:2015} proposed a solution based on queueing theory. The solution is designed for \emph{Nephele}\cite{Lohrmann:2014} streaming engine which has a master-worker style architecture. Similar to spark-streaming, a job is modelled as a DAG. It utilizes \emph{adaptive output batching}\cite{Warneke:2011} -- which is essentially a buffer with variable size -- to buffer outgoing messages emited from one stage to the other. Each task -- an executer that runs UDF -- is modelled as a G/G/1 queue. That is, the probability distributions of message inter-arrival and service times are unknown. In order to approximate these distributions, a formula proposed by \textcite{Kingman:1961} is used. From a bird's eye view, this solution seems promising. However, authors made two unconvincing assumptions that led us to abandon the proposal. First, worker nodes shall be homogeneous in terms of processing power and network bandwidth. Second, there should be an effective partitioning strategy in place in order to load balance outgoing messages between stages. In reality both assumptions rarely occur. Large scale stream processing clusters are build incrementally. Depending on workload, data skew does exist and imperfect hash functions are widely used by software developers.
\newpage
\textcite{Dutreilh2010} proposed a model-free reinforcement learning approach (\emph{Temporal difference} algorithm) with modified \emph{exploration} policy. The standard exploration policy for Q-Learning is $1-\epsilon$. Under this policy, the agent performs a random action with probably of $\epsilon$ and with probability of $1-\epsilon$, it adheres to an action proposed by optimal policy. Although the random action is necessary to explore unknown states, but it has sever consequences under streaming workloads. In some cases, it leads to unsafe states where SLOs are severely violated. Since streaming is heavily latency sensitive, this property is undesirable. Thus, author sought toward a heuristic-based policy proposed by~\textcite{Bodik:2009}. This policy is based on couple of key observations.
\begin{itemize}
\item It must quickly explore different states.
\item It should collect accurate data as fast as possible, to speedup training.
\item During exploration phase, the policy should be careful not to violate SLOs.
\end{itemize}
The aforementioned policy, works as follows.
\begin{itemize}
\item Initially, policy pushes the system towards its maximum capacity by removing as many worker VMs as possible, until some SLO is violated.
\item As soon as SLO is violated, it \emph{immediately} adds one VM to compensate SLO violation.
\item It keeps a pool of hot-standby worker VMs, to help recover from undesirable decisions.
\item Adding and removing VMs one at a time has a major benefit. It helps to approximate maximum throughput of a single worker VM. This approximation assists the policy to figure out, how much SLO would be impacted in case it adds or removes one virtual machine.
\item During grace period, measurements are discarded. The logic behind such a behaviour is that, during stabilization period metrics tend to be less accurate.
\end{itemize}
Some of the key concepts of this work, has been adopted by the author of this thesis.\newline

\textcite{Heinze:2015} 