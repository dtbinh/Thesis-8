\chapter{Related Work}
Dynamic resource scaling in cloud environments has been studied extensively in literature. As a naive implementation, there are two thresholds, namely \emph{upper bound} and \emph{lower bound}. However, such an implementation suffers from \emph{oscillating} decisions. In order to remedy this issue, \emph{grace period} shall be enforced. During this period, no scaling decision is made. \newline

\textcite{Hasan2012IntegratedAA} introduced four thresholds and two time periods. \lstinline$ThrUpper$ defines upper bound. \lstinline$ThrBelowUpper$ is slightly below \lstinline$ThrUpper$. Similarly, \lstinline$ThrLower$ defines lower bound and \lstinline$ThrAboveLower$ is slightly above the lower bound. In case, system utilization stays between \lstinline$ThrUpper$ and \lstinline$ThrBelowUpper$ for a specific duration, then cluster controller decides to take a scale-out action, by adding resources. On the other hand, if system utilization stays between \lstinline$ThrLower$ and \lstinline$ThrAboveLower$ for a specified duration, then controller decides to take scale-in action. Defining two levels of thresholds helps to detect workload \emph{persistency} and avoid making immature scaling decision. However, defining thresholds is a tricky and manual process, and need to be carefully done\cite{Dutreilh2010From}. It should be noted that, computation overhead of this approach is very low. \newline

\emph{RightScale} applies voting algorithm\cite{RightScale} among nodes to make scaling decision. In order for a specific action to be decided, majority of nodes should vote in favour of that action. Otherwise, no action is selected as a default action. Afterwards, nodes apply grace period to stabilize the cluster. The complexity of the voting process in trusted environments is in the order of $O(n^2)$, which leads to heavy network traffic among participants when cluster size grows. This approach is also categorized in threshold based approaches. Thus, it suffers from the same issue as mentioned above.\newline

\textcite{Herbst:2013} surveys many different auto-scaling techniques based on time-series analysis in order to forecast \emph{trends} and \emph{seasons}. \emph{Moving Average Method} takes the average over a sliding window and smooths out minor noise level. Its computational overhead is proportional to size of the window. \emph{Simple Exponential Smoothing}(SES) goes further than just taking average. It gives more weight to more recent values in sliding window by an exponential factor. Although it is more computationally intensive compared to moving average, it is still negligible. SES is capable of detecting short-term trends but fails at predicting seasons. These approaches are more specific instances of \emph{ARIMA} (Auto-Regressive Integrated Moving Average) which is a general purpose framework to calculate moving averages. However, time-series analysis is only suitable for stationary problems consist of recurring workload patterns such as web applications. In case of streaming (specially in multi-tenant environments), in which the workload is highly unpredictable, time-series analysis tends to be less useful. Additionally, more advanced forms of time-series analysis which are capable of forecasting seasons (such as \emph{tBATS Innovation State Space Modelling Framework}\cite{Alysha2011}, \emph{ARIMA Stochastic Process Modelling Framework}\cite{JSSv027i03}) are computationally infeasible for streaming workloads. 