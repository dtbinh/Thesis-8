\chapter{Related Work}
\label{related}
Dynamic resource allocation in cloud environments has been studied extensively in literature. In this chapter prior work will be discussed and explored. It is organized as follows. Section~\ref{related:thb} delves into threshold-based techniques. Section~\ref{related:tsa} investigates techniques based on time-series analysis. Section~\ref{related:qt} analyses techniques based on queuing theory. Section~\ref{related:rl} explores Reinforcement Learning techniques comprehensively. Finally, Section~\ref{related:sum} concludes.

\section{Threshold-Based Techniques}
\label{related:thb}

\textcite{Hasan2012IntegratedAA} proposed four thresholds and two time periods. \lstinline$ThrUpper$ defines upper bound. \lstinline$ThrBelowUpper$ is slightly below \lstinline$ThrUpper$. Similarly, \lstinline$ThrLower$ defines lower bound and \lstinline$ThrAboveLower$ is slightly above the lower bound. In case, system utilization stays between \lstinline$ThrUpper$ and \lstinline$ThrBelowUpper$ for a specific duration, then cluster controller decides to take a scale-out action, by adding resources. On the other hand, if system utilization stays between \lstinline$ThrLower$ and \lstinline$ThrAboveLower$ for a specified duration, then the central controller decides to take scale-in action. Furthermore, in order to prevent making \emph{oscillating} decisions, \emph{grace period} is enforced. During this period, no scaling decision is made. Defining two levels of thresholds helps to detect workload \emph{persistency} and avoids making immature scaling decision. However, defining thresholds is a tricky and manual process, and needs to be carefully done~\cite{Dutreilh2010}. It shall be noted that, computation overhead of this approach is very low.

\emph{RightScale}~\cite{RightScale} applies voting algorithm among nodes to make scaling decisions. In order for a specific action to be decided, majority of nodes should vote in favor of that specific action. Otherwise, no-action is elected as a default action. Afterwards, nodes apply grace period to stabilize the cluster. The complexity of the voting process in trusted environments is in the order of $O(n^2)$, which leads to heavy network traffic among participants when cluster size grows. This approach also suffers from the same issue -- accurately adjusting threshold values -- as other threshold-based approaches.

\textcite{Heinze:2015} proposed a novel threshold-based solution in the context of FUGU~\cite{Grandl:2014:MPC} -- a data stream processing framework. This techniques uses an adaptive window~\cite{Bifet:2007} to monitor the recent changes in workload pattern. In case a change in workload is detected, optimization component is activated and fed with recent short-term utilization history. Thereafter, the optimization component determines monetary cost of current system configuration and then simulates the cost of different scaling decisions. The \emph{latency-aware} cost function has the responsibility to calculate monetary cost of system configuration. The search function is an implementation of \emph{Recursive Random Search}~\cite{Ye:2003:RRS} algorithm which consists of two phases. First, in \emph{exploration} phase, the complete parameter space is explored to find a solution with minimum cost. In second phase -- \emph{exploitation phase} -- only specific parts of the parameter space which has been discovered in first phase, will be investigated. \textcite{Michal:2017} has implemented this techniques in the context of Spark Streaming. Thus, It is considered in evaluation scenarios.

\section{Time-Series Analysis Techniques}
\label{related:tsa}

\textcite{Herbst:2013} surveys different auto-scaling techniques based on time-series analysis in order to forecast \emph{trends} and \emph{seasons}. \emph{Moving Average Method} takes the average over a sliding window and smooths out minor noise level. Its computational overhead is proportional to size of the window. \emph{Simple Exponential Smoothing}(SES) goes further than just taking average. It gives more weight to more recent values in sliding window by an exponential factor. Although it is more computationally intensive compared to moving average, it is still negligible. SES is capable of detecting short-term trends but fails at predicting seasons. These approaches are more specific instances of \emph{ARIMA} (Auto-Regressive Integrated Moving Average) which is a general purpose framework to calculate moving averages. However, time-series analysis is only suitable for stationary problems consist of recurring workload patterns such as web applications. Additionally, more advanced forms of time-series analysis which are capable of forecasting seasons (such as \emph{tBATS Innovation State Space Modeling Framework}~\cite{Alysha2011}, \emph{ARIMA Stochastic Process Modeling Framework}~\cite{JSSv027i03}) are computationally infeasible for streaming workloads.

\textcite{Taft:2018} applied time-series analysis in the context of OLTP databases. The authors argue that reactive approaches don't fit to database world. By the time, auto-scaler component decides to scale-out, it is already too late for a database system. This premise comes from the fact that taking scaling actions in a database doesn't take place in timely manner. The database system has to replicate some of the records which is an additional burden on a heavily loaded system. Thus, database system must take proactive approach and take scaling decisions ahead of time. While this is convincing argument, the auto-scaler module depends on a couple of parameters that are hard to calculate in heterogeneous public cloud environments. First, target throughput of a single server. Second, shortest time to move all database records with single sender-receiver thread. While this might be feasible in some scenarios, on today's cloud environments with virtual machines hosted on heterogeneous physical nodes, getting a near-precise number is unconvincing. It worth noting that author assumed an approximately uniform workload distribution for all database nodes -- each database shard serves a fairly equal portion of total workload which is a questionable assumption.

\section{Queuing Theory Techniques}
\label{related:qt}

\textcite{Lohrmann:2015} proposed a solution based on queuing theory. The solution is designed for \emph{Nephele}~\cite{Lohrmann:2014} streaming engine which has a master-worker style architecture. Similar to Spark Streaming, a job is modeled as a DAG. It utilizes \emph{adaptive output batching}~\cite{Warneke:2011} -- which is essentially a buffer with variable size -- to buffer outgoing messages emitted from one stage to the other. Each task -- an executer that runs user defined function (UDF) -- is modeled as a G/G/1 queue. That is, the probability distributions of message inter-arrival and service time are unknown. In order to approximate these distributions, a formula proposed by \textcite{Kingman:1961} is used. From a bird's eye view, this solution seems promising. However, authors made two unconvincing assumptions that led us to abandon the proposal. First, worker nodes shall be homogeneous in terms of processing power and network bandwidth. Second, there should be an effective partitioning strategy in place in order to load balance outgoing messages between stages. In reality both assumptions rarely occur. Large scale stream processing clusters are built incrementally. Depending on workload, data skew does exist and imperfect hash functions are widely used by software developers.

\textcite{Zhang:2007} proposed a solution for multi-tiered enterprise applications based on regression techniques. Regression based models can absorb some level of uncertainty and noise by compacting samples. Each tier is modeled as G/G/1 queue and scaled differently compared to other tiers. The system has fixed number of users -- a principle known as \emph{closed-loop queuing network}. In order to calculate system workload -- incoming message rate -- and service time which is required by queuing models, the authors proposed to use Mean Value Analysis~\cite{Menasce:2004}. In order to simplify the queuing network, the system is modeled as a \emph{transaction-based} system with independent requests coming from clients. However, It is widely believed that multi-tiered enterprise applications are \emph{session-based} systems~\cite{Cherkasova:2002}. Each request from the same client depends on her previous request during a specific session.
 
\section{Reinforcement Learning Techniques}
\label{related:rl}

\textcite{Herbst:2017} surveys on state of the art techniques to predict future workload. It includes workload forecasting based on \emph{Bayesian Networks} (BN) and \emph{Neural Networks}. There are several issues with each of them that makes them unsuitable for streaming workloads. As an example, there is no universally applicable method to construct a BN. Furthermore, it requires collecting data and training the model offline. Neural networks suffer from the same issues. That is, it requires collecting samples and periodically training the model. For complex models, training phase is typically computationally infeasible which is conflicting with requirements of thesis.

\textcite{Tesauro2006} proposes a hybrid approach to overcome poor performance of online training. The system consists of two components: an online component based on queuing system combined with Reinforcement Learning component that is trained offline. The offline component is based on \emph{neural networks}. The authors model the data center as multiple applications managed under a single resource manager. Modeling streaming workloads as a queuing system has two problems. First, modeling is a complicated process and determining probability distributions requires domain knowledge. Second, it requires access to each node (so it can be modeled as a queue) which is currently not possible without modifying \lstinline$spark-core$ package. Since, it was one the requirements to provide a solution without making any modification to \lstinline$spark-core$, this work has been abandoned.

\textcite{Rao:2009:VRL} proposed to use Reinforcement Learning to manage resources consumed by virtual machines. It employs standard model-free learning, which is known as \emph{Temporal Difference}~\cite{rlIntro} or \emph{Sarsa} algorithm. The state space consists of metrics collected from virtual machines (CPU, RAM, Network IO, \dots). There is no global controller and each node decides based on its own Q-Table. As mentioned in literature, standard temporal difference has a slow convergence speed. In order to speedup bootstrap phase, Q-Table is initialized by values that were obtained during separate supervised training. Since this approach also relies on offline training, it wasn't adopted by this thesis.

\textcite{Enda:2012} proposed a parallel architecture to Reinforcement Learning. Standard model-free learning (Temporal Difference) is used. No global controller is involved and each node decides locally. In order to speed up learning, all nodes maintain two Q-Tables (local and global tables). Local table is learned and updated by each node. Whenever, an agent learns a new value for a specific state, it broadcasts it to other agents. The global table contains values received from other agents. Additionally, agent prioritize local and global tables by assigning weights to each table. Weights are factors that are defined by application developers. The final decision is the outcome of combining local and global tables. Although each node learns some part of the state space (which may overlap with other nodes), it is not applicable in the context of Spark Streaming. The assumption in this architecture is that, each node is operating autonomously without intervention from other nodes (such as web servers). In contrast, Spark is a centrally managed system. That is, all nodes running Spark jobs are supervised by a single master node (probably with couple of backup masters).

\textcite{Heinze:2014} implemented Reinforcement Learning in the context of FUGU~\cite{Grandl:2014:MPC} and compared it to threshold-based approaches. Each node, maintains its own Q-Table and imposes local policy without coordinating other nodes. This architecture can not be applied in the context of spark streaming, since Spark abstracts away individual nodes from the perspective of application developer. In order to decrease state space, the author applied two techniques. First, only system utilization is considered. Second, system utilization is discretized using coarse grained steps. To remedy slow convergence, the controller enforces a \emph{monotonicity constraint}~\cite{Herodotou:2011}. That is, if the controller decides to take scale-out action for a specific utilization, it may not decide scale-in for even worse system utilization. This feature has been adopted by this thesis.  

\textcite{CARDELLINI2018171} proposed a two level hierarchical architecture for resource management in Apache Storm~\cite{Storm}. There is a local controller on each node which is cooperating with the global controller. The local controller monitors each operator using different policies (threshold-based or Reinforcement Learning using temporal difference). In case, local controller decides to scale in or out an specific operator, it contacts the global controller and informs it about its decision. Then it waits to receive confirmation from the global controller. The global controller operates using a token-bucket-based policy~\cite{Valeria:2018} and has global view of cluster. It ranks requests coming from local controllers and either confirms or rejects their decisions. Although, this architecture seems to be a promising approach, however it has been implemented by modifying Storm's internal components. As mentioned above, this is in conflict with thesis's requirements.

In order to mitigate the problem of large state space in Reinforcement Learning, \textcite{Lolos:2017} proposed to start the agent from small number of coarse grained states. As more metrics are collected (and stored as historical records), agent will discover \emph{outlier} parameters (those parameters that are affecting agent more, CPU rather than IO as an example). Then, it partitions the affected state into two states and \emph{re-trains} newly added states using historical records. Both Temporal Difference and Value Iteration methods can be used as learning algorithm. Gradually, agent only focuses on some specific parts of the state space, since all parameters are not equally important. This approach, effectively reduces the size of state space. However, the trade-off is the storage cost in which historical metrics need to be stored. It worth noting that from the context of paper, storage cost (whether it is in-memory or on-disk and the duration of storing historical metrics) is unclear. Thus, this approach has been abandoned due to uncertainty.

\textcite{dutreilh:hal-01122123} proposed a model-based Reinforcement Learning approach for resource management of cloud applications. All virtual machines are supervised by a single global controller. Slow convergence is the bottleneck of model-free learning, in contrast to model-based learning. However, environment dynamics are not available at the time of modeling. Authors proposed to estimate these parameters as more metrics are collected and then switch to \emph{Value Iteration}~\cite{rlIntro} algorithm instead of \emph{Temporal Difference}. In short, statistical metrics are stored and updated for each visit of (old state, action, reward, new state) quadruple. As more samples are collected, statistical metrics become more accurate and can be directly used in \emph{Bellman} equation. Until enough measurements get collected, a separate initial reward function is used which is essentially the original reward function but with penalty costs removed. Furthermore, In order to reduce the state space -- tuple of [request/sec, number of VMs, average response time] -- there exists a predefined upper and lower bound for state variables and average response time is measured at granularity of seconds. This approach has been partially adopted by this thesis.

\textcite{Dutreilh2010} proposed a model-free Reinforcement Learning approach (\emph{Temporal difference} algorithm) with modified \emph{exploration} policy. The standard exploration policy for Q-Learning is $1-\epsilon$. Under this policy, the agent performs a random action with probably of $\epsilon$ and with probability of $1-\epsilon$, it adheres to an action proposed by optimal policy. Although the random action is necessary to explore unknown states, but it has sever consequences under streaming workloads. In some cases, it leads to unsafe states where SLOs are severely violated. Since streaming is heavily latency sensitive, this property is undesirable. Thus, author sought toward a heuristic-based policy proposed by~\textcite{Bodik:2009}. This policy is based on couple of key observations which has been adopted by this thesis:
\begin{itemize}
    \item It must quickly explore different states.
    \item It should collect accurate data as fast as possible, to speedup training.
    \item During exploration phase, the policy should be careful not to violate SLOs.
\end{itemize}

\section{Summary}
\label{related:sum}

In this chapter prior work on auto-scaling scaling has been discussed and evaluated. First, threshold-based approaches are investigated. Simple threshold-based approaches are intuitive and simple to understand by application developers and are widely supported by cloud providers. However, adjusting thresholds is a tricky and error-prone process. Then, time-series analysis techniques are explored. As confirmed by other authors, advanced seasonal forecasting is a computationally intensive process, which makes it less suitable for streaming workloads. Queuing theory approaches are suitable for stationary networks with a known probability distribution for workload and service time. Reinforcement Learning techniques has the benefit that it requires zero knowledge about the environment which helps to gradually adapt to changes in environment.