\chapter{Conclusion}
\label{conc}

Over-provisioning of resources is a common issue among modern cloud-based applications. The issue has been studied comprehensively in literature. It is specifically interesting in the context of streaming workloads with unpredictable workload patterns. Unpredictable workloads are getting common by increasing adoption of IOT style workloads. For such workloads, dynamic resource management is becoming a necessary component rather than a nice feature sitting around the corner. Previous research has led to a wide spectrum of dynamic resource managers with extensive list of features. Proposals vary from architectural to algorithmic aspects. In this thesis a dynamic resource manager has been implemented for Spark Streaming. 

Considering thesis goals has led to the solution proposed in this thesis. Since Spark has a Master-Slave architecture, this architecture has also been adopted by this thesis. The resource manager is implemented as a library which can be included by the Spark job. It subscribes to Spark's internal events and takes actions periodically. Since modifying Spark source code was prohibited, other architectures -- that require local agents on each node -- have been abandoned. From algorithmic point of view, Reinforcement Learning approach has been elected. Time-series analysis approaches are suitable for repetitive workloads. Queuing approaches are suitable for stationary systems. Threshold based approaches require manually specifying threshold values. Considering previous research implies that Reinforcement Learning approach is reasonably right choice that is capable of adopting dynamic workloads.

Reinforcement Learning consists of wide range of algorithms. As mentioned the focus of thesis was on streaming workloads which implies that decision making has to be done efficiently and in timely manner. As a consequence, those algorithms that require lengthy and intensive offline training were not adopted. This led us to two algorithms. Namely Temporal Difference -- Q-Learning --  and Value Iteration. A great aspect of Q-Learning is that it doesn't require any training at all which makes it perfectly suitable for streaming workloads. On the other hand, as confirmed by literature one of the negative aspects of Q-Learning is its slow learning process. This led us to two solutions. First, initializing the state space by sensible values which is independent of workload. Second, Value Iteration algorithm which requires not-so-intensive period of training phase.

Alongside the proposed state space initializer, a couple of optimizations have been implemented as well. First, a queue aware executor strategy which is derived from TCP Slow-Start algorithm has been implemented. It makes Scale-Out actions in large steps and Scale-In actions in small steps. Second, a monotonicity constraint has been adopted from previous research which to some degree prevents the Auto-Scaler from making undesirable decisions. Third, state space is discretized both in direction of latency and incoming messages. The implementation is extensible to a large degree. It is fairly easily to provide different implementation of state space initializer, policy function, reward function and executor strategy. Note that these components are not system wide parameters. Each job can easily provide its own implementation of these components at submission time.

Two workloads from DEBS 2014 that have high degree of unpredictability have been chosen to verify the implementation and evaluate it by changing configuration parameters. As can be seen, the implementation is able to achieve its goals. That is, it is able to optimize in two directions. First, optimizing for latency. Second, optimizing with respect to number of executors. At least based on aforementioned workloads it achieved better results than Spark's default dynamic resource manager.

In order to make the implementation suitable for vast number of workloads, many configuration parameters have been introduced. Evaluations demonstrate how sensitive Q-Learning approach is to configuration changes. However, on the negative side it makes it pretty cumbersome to find an optimal configuration. Note that, even by running many experiments in Evaluation section, it is still not possible to define optimal configuration. Although some empirical hints can be provided but it is just speculation -- not formally provable -- and to a large degree workload dependent. In other words, the configuration space is so large that it requires too many manual experiments to find out a fairly desirable configuration.

The issue of large configuration space remains mostly intact regardless of the underlying algorithm. Similar to sophisticated software projects, it will most probably suffer from having too many configuration parameters. As a consequence, the problem of dynamic resource management has been shifted from design space to configuration space. The author of this thesis, believes this is the right direction for future research. That is, an approach that finds the optimal configuration from a huge multi dimensional configuration space which in turn leads to optimal runtime.

