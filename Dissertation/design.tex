\chapter{Design and Implementation Detail}
\label{design}

\section{Introduction}
\label{des:intro}

In this chapter design and project structure of implemented techniques will be introduced and discussed. Two techniques have been selected to be implemented. This chapter is organized as follows. Section~\ref{des:choose} explains selected techniques and the reasoning behind it. In this section, selected techniques are explained at theoretical level. Section~\ref{des:proj} describes the structure of the implementation for Spark Streaming in detail. Section~\ref{des:conf} describes the configuration parameters required to launch the implementation. Finally, Section~\ref{des:conc} concludes this chapter.

\section{Choosing Auto-Scaling Techniques}
\label{des:choose}

As discussed in Section~\ref{ias:alg-fam}, Auto-Scalers can be categorized into 5 groups.
\begin{description}[leftmargin=0pt]
    \item[Threshold-Based] In this approach, one or multiple threshold values are defined~\cite{Hasan2012IntegratedAA} which specifies the behavior of the Auto-Scaler when the system load goes beyond these thresholds. However, manually defining thresholds is a tricky process~\cite{Dutreilh2010}. Spark's default dynamic resource manager~\cite{spark} and Online Parameter Optimization~\cite{Heinze:2015} are partially based on this approach and they will be evaluated in Section~\ref{eval}.
    \item[Time-Series Analysis] In this approach, a history window of workload -- depending on prediction accuracy -- is considered to predict future workload. However, in case the workload is changing in an unpredictable way -- which is not an unusual phenomena in streaming applications -- then this approach becomes less useful. As confirmed by DEBS grand challenges~\cite{debs2014}~\cite{debs2015}~\cite{debs2016}, the general assumption for streaming applications is that the workload is unpredictable. Due to this limited applicability, no time-series technique is implemented.
    \item[Queuing Theory] In this approach, the system is modeled as a queue network. Since queuing parameters -- message inter-arrival and service rates --  are static, they have to be re-calculated in a timely manner. One of the novel implementations is done by~\textcite{Lohrmann:2015}. However, authors made two unconvincing assumptions. First, worker nodes shall be homogeneous in terms of processing power and network bandwidth. Second, there should be an effective partitioning strategy in place in order to load balance outgoing messages between stages. In reality both assumptions rarely occur. Large scale stream processing clusters are built incrementally. Depending on workload, data skew does exist and imperfect hash functions are widely used by software developers. Furthermore, as confirmed by~\textcite{Rajarshi:2005}, queuing models are very complicated to build and less adaptive to changing environments. As a consequence, no queuing theory technique is implemented.
    \item[Reinforcement Learning] Reinforcement Learning has shown its capabilities to adapt ever changing environments. However, some of the proposed solutions are conflicting with the requirement of this thesis as discussed in Section~\ref{prob-def}. The following summarizes the reasoning why each proposal is accepted or rejected by this thesis.
    \begin{itemize}
        \item \textcite{Herbst:2017} proposed a solution based \emph{Bayesian Networks}. There are two major problems with this proposal. First, it needs sampling and offline training which is inapplicable for changing streaming workloads. Second, in case the model is complex, the training phase is long and computationally infeasible for streaming workloads. As mentioned, both issues are conflicting with thesis requirements.
        \item \textcite{Tesauro2006} proposes a hybrid approach to resolve performance issues of online training which consists of two components. First an online component based on queuing theory. Second, Reinforcement Learning component that is trained offline. This proposal models each node as a queue. However, the only way to apply this technique is to modify \lstinline$spark-core$ package. Thus, this solution is not implemented.
        \item \textcite{Rao:2009:VRL} proposed a solution to manage virtual machine resources. However, it is also based on offline training and sampling which is obtained from a separate supervised training phase. Thus, it's not applicable for dynamic streaming workloads. 
        \item \textcite{Enda:2012} proposed a parallel architecture to Reinforcement Learning without any global controller involved. Nodes (RL agents) have two tables. Local table is trained by each node separately. Global table contains values learned by other nodes. When an agent learns anything new, it broadcasts it to other nodes. From theoretical point of view, this solution might seem feasible, since parallel learning speeds up initialization process. However, Spark has a single global controller. Applying this technique to Spark requires heavy modification to \lstinline$spark-core$ package. 
        \item \textcite{Heinze:2014} implemented Reinforcement Learning in the context of FUGU~\cite{Grandl:2014:MPC}. Each node, maintains its own Q-Table and imposes local policy without coordinating with other nodes. Although this architecture is not applicable for Spark, but its core idea -- \emph{Temporal Difference}~\cite{rlIntro} algorithm-- is applicable. Thus, this thesis implemented this proposal by adopting it to Spark architecture. Refer to Section~\ref{des:temp} for theoretical background and Section~\ref{des:proj} for implementation detail.
        \item \textcite{CARDELLINI2018171} proposed a two level hierarchical architecture for resource management in Apache Storm~\cite{Storm}. Local controller applies local policy on each node and coordinates with the global controller for confirmation of its actions. Although, this architecture seems to be a promising approach, however it has been implemented by modifying Storm's internal components. As mentioned above, this is in conflict with thesis's requirements.
        \item \textcite{dutreilh:hal-01122123} proposed a model-based Reinforcement Learning approach for resource management which is based on a global controller. In order to overcome the slow convergence of model-free learning, authors proposed to estimate environment dynamics based on collected samples at runtime. Then it switches to \emph{Value Iteration}~\cite{rlIntro} algorithm instead of \emph{Temporal Difference}. This approach has also been partially adopted by this thesis. Refer to Section~\ref{des:val} for theoretical background and Section~\ref{des:proj} for implementation detail.
    \end{itemize}
    \item[Control Theory] Techniques based on Control Theory are also promising for elastic data streaming. Because it monitors input and output of the application, it can respond to workload changes very fast and adapt the system if necessary. Thus it is perfectly capable of handling dynamic environments. The comparison between Reinforcement Learning and Control Theory techniques is left for future work.
\end{description}
As mentioned, two techniques -- \emph{Temporal Difference} and \emph{Value Iteration} -- will be implemented in this thesis. In next two sections, theoretical foundation of these algorithms will be laid out. Noteworthy to mention, these sections are heavily inspired by~\textcite{rlIntro}. Furthermore, Section~\ref{related} discusses more techniques and the reasoning why they have been accepted or rejected by this thesis.

\subsection{Temporal Difference}
\label{des:temp}
Section~\ref{ias:alg-rl} explains the basics of Reinforcement Learning. Temporal Difference (TD) learning is one of the foundational algorithms of Reinforcement Learning. It can learn from applying experience without having prior knowledge about environment's dynamics. This property is potentially useful for data stream processing systems in which the incoming workload is changing without any particular pattern.

Before starting to dig into details, some formal notions shall to be explained. An \emph{episode} is series of experiences that is taken by the agent. At each time step~$t$, the agent moves from one \emph{state}~$S_t$ to another state~$S_{t+1}$. A \emph{policy} $\pi$ defines the action~$A_t$ that should be taken by the agent at each state. Followed by each action, the agent receives a \emph{reward}~$R_{t+1}$ from the environment. Note that, the agent will receive reward for its corresponding action at next state~$S_{t+1}$. Thus, an episode can be viewed as Equation~\ref{des:eq:episode}.
\begin{equation}
\dots\,S_t,\,A_t,\,R_{t+1},\,S_{t+1},\,A_{t+1},\,R_{t+2},\,S_{t+2},\,A_{t+2},\,R_{t+3}\,\dots
\label{des:eq:episode}
\end{equation}

An episode does not necessarily have a \emph{terminal} state. In some environments -- like data stream processing -- an episode is a never ending sequence of experiences. Prior to taking an action, the agent has an estimate of \emph{expected reward}~$V$ which specifies future reward if it follows the same actions returned by policy~$\pi$ from that specific state. Thus, it is referred as $V_\pi$ which translates to expected future reward under policy~$\pi$. During the episode, the agent tries to \emph{maximize} the reward that it received from the environment. Any policy that leads to maximum possible reward is called the \emph{optimal} policy~$\pi^*$ and its corresponding estimate of future reward is referred to as~$V^{*}_{\pi}$.

Unlike Monte Carlo~\cite{rlIntro} methods that requires agent to wait until the end of episode, TD approaches only need to wait until the end of next time step to get a feedback from environment. Then, the agent updates corresponding estimate of previous state. Thus, the most simple TD approach can be formulated as Equation~\ref{des:eq:std}. Equation~\ref{des:eq:std} contains two important parameters of TD.
\begin{equation}
V(S_t) \longleftarrow (1-\alpha)V(S_t) + \alpha\big[R_{t+1} + \gamma\,V(S_{t+1})\big]
\label{des:eq:std}
\end{equation}
\begin{itemize}
	\item $\bm{\alpha}$ denotes \emph{Learning Factor}. It specifies, how much the agent shall learn from new experiences. A higher $\alpha$ means learning with a faster pace. It also leads to forgetting history faster. On the other hand, a lower $\alpha$ leads to slower learning process which means the agent trusts its experience history more -- it gives more weight to history rather than new experiences. $\alpha$ is defined as a number between (0,1].
	\item $\bm{\gamma}$ denotes \emph{Reward Factor} or \emph{Discount Factor}. It specifies, whether the agent shall optimize for \emph{future} or \emph{immediate} reward. A higher $\gamma$ leads to optimizing for future reward, whereas a lower $\gamma$ leads to optimizing for immediate reward. It is defined as a number between (0,1). As defined by Equation~\ref{des:eq:reward}, reward is cumulating at each time step which causes the sum to grow infinitely. In order to prevent infinite reward problem, it is crucial to define $\gamma$ as a number less than one to force convergence.
	\begin{equation}
	R_t = r_{t+1}+\gamma r_{t+2}+\gamma^2 r_{t+3}+\dots=\sum_{0}^{\infty}\gamma^k r_{t+k+1}
	\label{des:eq:reward}
	\end{equation}
\end{itemize}

Since expected future reward is defined per action, Equation~\ref{des:eq:std} changes slightly and becomes as Equation~\ref{des:eq:qtd}. This update is applied after every transition from a nonterminal state $S_t$. In case $S_{t+1}$ is terminal, then $Q(S_{t+1},\,A_{t+1})$ is set to zero. As a consequence, each experiment can be defined with a quadruple of (State, Action, Reward, State) or $(S_{t},\,A_{t},\,R_{t+1},\,S_{t+1})$. This method is known as \emph{Sarsa} algorithm and named \emph{Q-Learning} because of the Q-Table used in the equation.
\begin{equation}
Q(S_t,\,A_t) \longleftarrow (1-\alpha)Q(S_t,\,A_t) + \alpha\big[R_{t+1} + \gamma\,Q(S_{t+1},\,A_{t+1})\big]
\label{des:eq:qtd}
\end{equation}



Algorithm~\ref{des:a:ql}\footnote{The algorithm has been taken from~\textcite{rlIntro}} describes this procedure.
\begin{algorithm}[h]
	\DontPrintSemicolon
	
	\textup{Algorithm parameters: $\alpha \in (0,1]$, $\gamma \in (0,1)$}\;
	\textup{Initialize $Q(s,\,a)$ for all $s \in S$, $a \in A(s)$} arbitrarily or from previous episodes\;
	\BlankLine
	\Repeat{\textup{End of episode or $S$ is terminal}} {
		\textup{Take action $A$ based on policy $\pi$ derived from $Q$ table}\;
		\textup{Observe reward $R$ and land in state $S^\prime$}\;
		\textup{Choose $A^\prime$ from $S^\prime$ using the same policy $\pi$}\;
		\BlankLine
		$Q(S,\,A) \gets (1-\alpha)Q(S,\,A) + \alpha\big[R + \gamma\,Q(S^\prime,\,A^\prime)\big]$\;
		\BlankLine
		$S \gets S^\prime$\;
		$A \gets A^\prime$
	}
	\caption{Q-Learning Work-Flow}
	\label{des:a:ql}
\end{algorithm}

\subsection{Value Iteration}
\label{des:val}

\section{Project Structure}
\label{des:proj}

\section{Configuration}
\label{des:conf}

\section{Conclusion}
\label{des:conc}