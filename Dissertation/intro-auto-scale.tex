\chapter{Introduction to Auto-Scaling}
\label{intro-auto-scale}

\section{Introduction}
\label{ias:intro}

As mentioned in Chapter~\ref{prob-def} the key characteristic of cloud environments is emph{elasticity} behavior. However, manually adjusting resources are is not effective approach to exploit this feature. Hence, we need to automate this procedure with minimal human intervention. This chapter introduces foundations of Auto-Scaling techniques. Different techniques and architectures will be discussed from a high level standing point. It shall be noted that, this chapter has been heavily inspired by work done by~\textcite{Lorido-Botran2014}.

\section{Basic Concepts}
\label{ias:basics}

The ultimate goal of an Auto-Scaling system is to automate the process of acquiring and releasing \emph{resources} in order to minimize the \emph{cost} with minimum violation of \emph{service level objectives} (SLO). However, \emph{resource} is a broad and context-dependent term. It refers to any form processing engine that provides application developers some form of computation power. This general purpose definition is broad enough to capture different kinds. In most cases it, it means virtual machines allocated by cloud provider. In more modern distributed systems, a resource refers to \emph{container}s like Google Kubernetes~\cite{kuber}. However, a resource might be as simple as a single process or thread.

The term \emph{cost} refers to any form of expenditure that users pay in order to acquire a resource. It doesn't necessarily mean \emph{monetary} cost. It can also refer to numerical values of resources, like number of virtual machines or number of running processes. Although minimizing cost is the ultimate goal of any Auto-Scaler system, not in all cases cost reduction is desirable. It should be achieved with respect to defined \emph{service level objectives}.

\emph{Service Level Objective}s are any predefined rules that shall be respected during application runtime. The following, defines a couple of SLO definitions for different applications:
\begin{itemize}
    \item 99 percentile round-trip latency of requests in a web application should be less than 150 milliseconds.
    \item All committed records in master database must be replicated with a maximum delay of 5 milliseconds.
    \item All messages pushed by a producer, should be processed by respective consumers in less than 5 minutes.
    \item At least 95\% of images published in the last 24 hours should be served by cache servers.
\end{itemize}
Service Level Objectives are typically determined and defined by business requirements. Defining effective and meaningful SLOs is a challenge on its own. However, it is out of the scope of this thesis. 

From a high level point of view, Auto-Scaling is a \emph{trade-off} amongst cost and violation of SLOs. Resource \emph{under-provisioning} will degrade performance and leads to SLO violations, while on the other hand, resource \emph{over-provisioning} leads to idle resources, hence unnecessary costs. To make an effective decision, an Auto-Scaler needs to consider application and its environment:
\begin{itemize}
    \item \textbf{Infrastructure pricing model}. Some infrastructure providers charge customers hourly. That is, if customer acquires a resource at 10:30AM and releases it at 11:30AM, the customer is charged for two hours. Some other service providers might charge on minute basis. Pricing model has a huge impact on decisions made by Auto-Scaler system, since it makes some decisions pointless.
    \item \textbf{Service level objectives}. Each application has its own set of objectives that should be adhered by Auto-Scaling system. These SLOs might be defined and applied at \emph{soft} and \emph{hard} levels. Violating an SLO at soft level is not critical, albeit alarming. However, violating a resource at hard level is a critical issue and is a negative point for an Auto-Scaler.
    \item \textbf{Acquire/Release delay}. Depending on type of the resource, it might take some time for the resource to become responsive and ready to process user requests. For example, booting a virtual machine typically takes couple of minutes, whereas launching a container takes time in order of seconds. An Auto-Scaler shall consider whether acquiring and releasing a heavy weight resource in a \emph{zig-zag} manner worth the overhead or not.
    \item \textbf{Unit of allocation}. In some cases, it might be beneficial to allocate multiple instances of a same resource at once. This might be due to the startup and initialization overhead or it might be the case that Auto-Scaler predicted a huge load spike in near future.
\end{itemize}

Auto-Scaling can be done with different techniques and strategies. The remainder of this chapter is organized as follows. Section~\ref{ias:arch} defines general architecture of an Auto-Scaler. Section~\ref{ias:actions} clarifies which sort of actions can be applied by an Auto-Scaler. Section~\ref{ias:taxonomy} classifies different techniques and briefly explains each category. Finally, section~\ref{ias:conc} concludes.

\section{Generic Auto-Scaler Architecture}
\label{ias:arch}

Figure~\ref{fig:auto-scaler-arch} illustrates generic architecture of an Auto-Scaler. This architecture is broad enough to capture different kinds of applications.
\begin{figure}[ht]
    \centering
    \includegraphics[clip, trim=3cm 12.5cm 2.5cm 2.5cm]{auto-scaler-arch.pdf}
    \caption{General Auto-Scaler Architecture}
    \label{fig:auto-scaler-arch}
\end{figure}
First, responsibilities of different components shall be clarified. Table~\ref{tab:auto-scaler-sum} summarizes all components of the system.
\begin{table*}[h!]
    \begin{tabular}{ll}
        \toprule
        \textbf{Component} & \textbf{Description}\\
        \midrule
        Clients & Users of the application which might be applications on their own\\
        Load Distributor & Distributes incoming requests to application instances\\
        Application & Runs business logic defined by developer\\
        Metrics Engine & Monitors and collects metric from application and provides it to other components\\
        Infrastructure API & Provides API to adjust (acquire/release) resources\\
        Auto-Scaler & Runs the auto-scaling algorithm based on metric collected by Metrics Engine\\
        \bottomrule
    \end{tabular}
    \centering
    \caption{Auto-Scaler components summary}
    \label{tab:auto-scaler-sum}
\end{table*}
An Auto-Scaling system typically consists of following sub-components.
\begin{description}[leftmargin=0pt]
    \item[Clients] Most kinds of applications, typically have some form of client that sends requests to the system and either waits to get a reply or operates under \emph{fire-and-forget} strategy. It shall be noted that, a client is not necessarily an end user. In today's modern distributed applications, an application might be client of another application.
    \item[Load Distributor] In order to provide some degree of \emph{transparency}, usually clients connect to a Load Distributor component. It is the responsibility of the Load Distributer that \emph{proxies} client request to application. Load Distributor is also a generic component and might represent different technology in read-world applications. In the context of a web application, it might be an HTTP load balancer. Even a \emph{Message Broker} can also be represented as a form of Load Distributer. It shall be noted that Load Distributor itself can be replicated or sharded for \emph{high availability} or \emph{scalability} reasons.
    \item[Application] The application component runs the business logic. This architecture doesn't impose any limitation on application architecture. It might be a simple stateless web application. It might access to a back-end cache or database service. It might push some messages to a message broker, as a result of client request. It might be just a simple process consuming messaging provided by a message broker. It might forward client requests to other applications for further processing.
    \item[Infrastructure API] Typically, when an Auto-Scaler system decides to take any action, it doesn't touch the application directly. In order to provide \emph{separation of concerns}, this responsibility is handed over to infrastructure via an API provided by resource/service provider. An important issue that shall be noted here is that, service provider may schedule resource changes and execute them some time later. Thus, resource changes might not take effect immediately at the moment Auto-Scaler requests them. This fact shall be considered by Auto-Scaler system.
    \item[Metric Engine] Auto-Scaler system needs to have a good insight on current status of the application and incoming requests. Metric engine -- known as \emph{monitoring engine} takes the responsibility of measuring and collecting different aspects of the application. The term \emph{metric} refers to any form of measurable aspect of an application or its environment. \textcite{Ghanbari:2011} has defined and proposed a list of different type of metrics that could be exploited for different purposes.
    \begin{itemize}
        \item \textbf{Hardware} dependent metrics such as CPU usage, disk access time, memory usage, network bandwidth usage, network latency.
        \item \textbf{Operating System} provided metrics such as CPU-time, page faults, real memory.
        \item \textbf{Load balancer} provided metrics such as size of request queue length, session rate, number of current sessions, transmitted bytes, number of denied, requests, number of errors.
        \item \textbf{Web server} provided metrics such as transmitted bytes and requests, number of connections in specific states (e.g. closing, sending, waiting, starting, \dots).
        \item \textbf{Application server} provided metrics such as total threads count, active threads count, used memory, session count, processed requests, pending requests, dropped requests, response time.
        \item \textbf{Database server} provided metrics such as number of active threads, number of transactions in a particular state (e.g. write, commit, roll-back, \dots).
    \end{itemize}
    Since storing and reporting metrics has its own overhead, typically metrics engine aggregates collected values at different scale depending on how fresh it is. Rationally, fresh values are more important for Auto-Scaling system. As an example, it might provide near real time values for about 15 minutes. Then, for the last 5 hours, collected values are aggregated by a window of one minute. For last two days, it is aggregated by a window of 15 minutes and finally, for any record older than last two days, it is aggregated on hourly basis. Whether these aggregated values are sufficient for Auto-Scaler to make an accurate decision is out of the scope of this thesis. However, it may be a good idea to empirically adjust this system until it fits the requirements of Auto-Scaler system.
    \item[Auto-Scaler] This component is the core of Auto-Scaling system. It consists of different techniques. Typically, an Auto-Scaler works in \emph{consecutive rounds}. First, it collects metrics from Metrics Engine and offloads them to one or multiple technique implementations. It shall be noted that, an Auto-Scaler might utilize different implementation of techniques simultaneously -- for different stages of the application as an example. Even it might utilize a single implementation under different configurations. This architecture does not impose any limit on the order of techniques. Then, based on some preferences or ordering mechanism, it chooses the \emph{final decision}. Finally, it requests the Infrastructure API to change the number of resources. Naturally the assumption is that, infrastructure provider should be able to adjust the required or released resources. However, in some case it might not be able to fulfill the task. Thus, Auto-Scaler shall wait for a response to check whether the request has been successfully fulfilled or not. 
\end{description}
For the sake of understandability, Algorithm~\ref{a:as-workflow} describes the above procedure in pseudo code.
\begin{algorithm}[ht]
    \DontPrintSemicolon
    
    \SetKwFunction{GCMFME}{GetCurrentMetricsFromMetricsEngine}
    \SetKwFunction{GD}{GetDecision}
    \SetKwFunction{GFD}{GetFinalDecision}
    \SetKwFunction{RIA}{RequestInfrastructureAPI}
    \SetKwFunction{error}{LogError}
    \SetKwFunction{factory}{InstantiateTechnique}
    
    \SetKwArray{impls}{implementations}
    \SetKwData{impl}{impl}
    \SetKwArray{dec}{decisions}
    \SetKwData{finaldec}{finalDecision}
    \SetKwData{reply}{reply}
    \SetKwData{currentValue}{currentValue}
    
    \tcp{different implementations of techniques}
    \impls $\gets$ [] \;
    \tcp{decision of each implementation}
    \dec $\gets$ [] \;
    \tcp{final decision of Auto-Scaler}
    \finaldec $\gets$ null\;
    \BlankLine
    \tcp{instantiate as many techniques as required}
    \For{$i \gets 0$ \KwTo $n$}{
        \impls{i} $\gets$ \factory{i}
    }
    \BlankLine
    \Repeat{\textup{Auto-Scaler is running}} {
        \tcp{load monitoring data from metrics engine}
        \currentValue $\gets$ \GCMFME{} \;
        \BlankLine
        \tcp{initialize decisions}
        \dec $\gets$ [] \;
        \For{$i \gets 0$ \KwTo $n$}{
            \impl $\gets$ \impls{i} \;
            \dec{i} $\gets$ \GD{\impl, \currentValue}
        }
        \BlankLine
        \tcp{calculate final decision based on some weight or ordering mechanism}
        \finaldec $\gets$ \GFD{\dec, \currentValue} \;
        \BlankLine
        \tcp{request infrstructure API to adjust resources}
        \reply $\gets$ \RIA{\finaldec}\;
        \BlankLine
        \tcp{in case infrastructure reject request, warn developers}
        \If{\reply = ReplyStatus.REJECT}{
            \tcp{issue a warning}
            \error{"request can not be fulfilled"}
        }
    }
    \caption{General work-flow of an Auto-Scaler}
    \label{a:as-workflow}
\end{algorithm}

\section{Actions}
\label{ias:actions}

In each round an Auto-Scaler decides to take an action and request that specific action to Infrastructure API. Table~\ref{tab:actions-sum} summarizes feasible actions for an Auto-Scaler.
\begin{table*}[h!]
    \begin{tabular}{ll}
        \toprule
        \textbf{Action} & \textbf{Description}\\
        \midrule
        Scale-In & Remove/Release one or more resources\\
        Scale-Out & Acquire/Add one or more resources\\
        No-Action & Do nothing\\
        \bottomrule
    \end{tabular}
    \centering
    \caption{Summary of feasible actions}
    \label{tab:actions-sum}
\end{table*}
This thesis, assumes three possible actions.
\begin{itemize}
    \item \textbf{Scale-In}. This action implies that Auto-Scaler has decided to remove one or more resources. It doesn't necessarily means, Infrastructure API will be able to remove all the requested resources. Consequently, Infrastructure API might be able to remove any subset of resources among those that were requested to remove. In this thesis, we assume Infrastructure API offers such a behavior to notify Auto-Scaler about the actual removed resources either \emph{synchronously} in response to Auto-Scaler's request or via an \emph{asynchronous} API provided by Auto-Scaler.
    \item \textbf{Scale-Out}. This action implies that Auto-Scaler has decided to add one or more resources. Similar to Scale-In action, Infrastructure API might be able to fulfill the request or not. In case, it doesn't have enough resources to allocate, this incident should be reported to Auto-Scaler component. This thesis assumes this behavior. Response shall propagate back to Auto-Scaler similar to steps described in Scale-In action.
    \item \textbf{No-Action}. This action implies that Auto-Scaler has decided to do nothing but stay with the same number of resources as last round. This is a kind of \emph{no-operation}.
\end{itemize}
It's noteworthy that in all cases, Auto-Scaler is allowed to store any history of actions taken so far. In fact, it is a special category of Auto-Scalers known as \emph{stateful} Auto-Scalers. Refer to section~\ref{ias:taxonomy} for further discussion and explanation on taxonomy of Auto-Scalers.

Another aspect of taking an action is that, Auto-Scaler is allowed to Scale-In/Out \emph{horizontally} or \emph{vertically} in each round independent of previous rounds. Horizontal Scale-In/Out refers to a category of actions that acquires or releases resources in parallel to each other. For example, in the context of a web application, adding or removing one or more virtual machines is considered as a horizontal scaling action. While on the other hand, Auto-Scaler might decide to just scale by adding hardware resources. For example, it might decide to add more RAM or remove couple of CPU cores in one specific virtual machine. This kind of scaling action is considered as vertical scaling action.

Actions are not necessarily applied at a constant rate. Auto-Scaler is in full charge of taking actions at \emph{exponential} rates. For example, in consecutive rounds, an Auto-Scaler can decide to acquire 1, 2, 4, 8, 16 virtual machines per round. This also applies for Scale-Out actions. Nothing hampers an Auto-Scaler from changing rate of scaling actions in each round. It might even decide to apply different rates for different stages of the application like \emph{startup} phase, or \emph{near-ending} phase, etc.

Last but not least, an Auto-Scaler might decide to apply a \emph{grace period} after taking an action independent of previous rounds. A grace period is a time frame, in which Auto-Scaler does not take any further action in order to let cluster of resources stabilize. Similar to action rates, grace period can also be applied at different rates. For example, Auto-Scaler might wait for 10, 20, 40 seconds after taking Scale-Out action in consecutive rounds.

\section{Taxonomy of Auto-Scaling Techniques}
\label{ias:taxonomy}

Auto-Scalers can be modeled and classified in different categories. In a sense, it is a multi-dimensional space of features and characteristics. Table~\ref{tab:taxonomy-sum} lists and summarizes different dimensions of Auto-Scaling. In the rest of this chapter each dimension is described and explored in details.
\begin{table*}[h!]
    \begin{tabularx}{\textwidth}{lX}
        \toprule
        \textbf{Dimension} & \textbf{Description}\\
        \midrule
        \emph{Schedule-based} versus \emph{Rule-based} & Whether Auto-Scaler applies decisions manually or based on predefined rules.\\
        \emph{Reactive} versus \emph{Proactive} & Whether Auto-Scaler reacts to workload changes or predict future workload ahead of time.\\
        \emph{Execution} mode & Whether Auto-Scaler is central component or operating in a distributed fashion.\\
        \emph{Algorithm} family & Which family of algorithms is applied to make the decision.\\
        \bottomrule
    \end{tabularx}
    \centering
    \caption{Summary of Auto-Scaling dimensions}
    \label{tab:taxonomy-sum}
\end{table*}
\subsection{Schedule-Based versus Rule-Based}
\label{ias:sched-rule}

Some applications have a very basic cyclic workload pattern on daily basis which could be manually modeled as \emph{cron} style jobs. Schedule-based systems can not adopt to unplanned workload changes. Since this type of Auto-Scalers are in conflict with thesis requirements, it won't be studied in this thesis.

On the other side of the extreme, there exists \emph{rule-based} Auto-Scalers. These Auto-Scalers take actions based on set of rules defined by application developers inspired by business requirements. Each rule is based on one or multiple \emph{constraints}. Each constraint consists of one or a set of conditions around some variable. Variables can be defined by \emph{application} -- number of tweets per second, as an example -- or by its environment -- CPU utilization, for example. For example, if average network bandwidth is more than 80\% of maximum available bandwidth for 10 minutes, then take Scale-Out action.

There are different criteria to calculate defined variables. Some applications might define minimum or maximum values for variables. In some other cases, average values might be useful. Furthermore, average/minimum/maximum values might be defined for a window of time or number of occurrences of specific event. Time or event windows in turn could be defined as a \emph{static} window -- where it moves with a fixed interval -- or as a \emph{sliding} window -- where it smooths over elements.
\subsection{Reactive vs Proactive}
\label{ias:react-proact}

In another dimension, Auto-Scalers can be partitioned into two categories. \emph{Reactive} approaches monitor the workload in order to find a meaningful change. Thereafter, they apply some algorithm to figure out the final decision. To view different family of algorithms, refer to section~\ref{ias:alg-fam}. It's noteworthy to express that, in some applications by the time a reactive Auto-Scaler decides to take some action, it might be too late. In other words, taking an action in an already overloaded application might not be desirable in some cases. \textcite{Taft:2018} argues that applying reactive approaches in the context of OLTP databases is not desirable. This leads us to another class of Auto-Scalers known as \emph{proactive} Auto-Scalers.

\emph{Proactive} Auto-Scalers try to predict \emph{future} workload ahead of time before facing workload spikes. Hence, they are called as \emph{predictive} approaches. Whether the term \emph{future} is defined as near future or long term future depends on the type of Auto-Scaler. This has the advantage that, by the time load spike occurs, the required resources are already available, warmed-up and ready to respond user requests. As mentioned in section~\ref{ias:basics}, allocating some resources takes some time to become fully initialized. For example, adding a database replica is not an immediate action. It takes some time re-replicate database records and validate the replicas. Thus, for some scenarios, even though reactive approaches might seem to be sufficient, but due to initialization latency it is not an applicable approach.

\subsection{Execution Mode}
\label{ias:exec-mode}

The generic architecture which is described in section~\ref{ias:arch} does not impose any limit on the operation and execution mode of the Auto-Scaler. In other words, Auto-Scaler itself might run as a \emph{multi-instance} application. There are different types of proposed execution modes. But they can be classified in following generic groups without loosing generality. Table~\ref{tab:exec-mode} summarizes operation modes. 
\begin{table*}[h]
    \begin{tabularx}{\textwidth}{lX}
        \toprule
        \textbf{Execution Mode} & \textbf{Description}\\
        \midrule
        Global Controller & At a specific point in time, a single controller is responsible of taking actions.\\
        Distributed Without Coordination & Auto-Scalers operate independent of each other without performing any form of coordination.\\
        Distributed Coordinated & Auto-Scalers perform in distributed mode, but they are allowed to communicate and cooperate with each other.\\
        Hierarchical Controllers & A hierarchy of controllers cooperate with each other to make final decision.\\
        \bottomrule
    \end{tabularx}
    \centering
    \caption{Summary of execution modes}
    \label{tab:exec-mode}
\end{table*}
\begin{description}[leftmargin=0pt]
    \item[Global Controller] In this architecture, Auto-Scaler runs and controls the application as a \emph{central} or \emph{master} component. One or multiple \emph{backup} or \emph{slave} controllers might also accompany master node. In case master controller fails, one of the backup controllers kicks in and takes over the responsibility. In this model, it's the responsibility of master node to make decisions and execute actions. To detect failure of master controller, there exist already well established \emph{cluster coordination} utilities like Apache Zookeeper~\cite{zk} or CoreOS Etcd~\cite{etcd}. Figure~\ref{fig:auto-scaler-master-backup} depicts a variation of original Auto-Scaler architecture that performs under master-slave model coordinated by a Zookeeper ensemble.
    \item[Distributed Without Coorindation] In this architecture, typically Auto-Scaler runs alongside the application instances and each Auto-Scaler instance only \emph{controls} the \emph{local} application. The major difference against global controller is that, in this model each Auto-Scaler makes decision at its own will without any form of coordination with other Auto-Scalers. It's usually the most scalable solution when size of the cluster grows. However, since each Auto-Scaler instance only controls the local application, it might lead to sub-optimal decisions because local controllers lacks global view of the cluster. Figure~\ref{fig:auto-scaler-dist-wo-coord} illustrates this architecture.
    \item[Distributed Coordinated] This architecture is similar to previous model. However the difference is that, Auto-Scaler components coordinate with each other over a communication channel to gather more information. The amount of communicated information heavily depends on underlying algorithm. In some scenarios only neighbor nodes are contacted. In other cases, all nodes might communicate with each other in order to achieve consensus. Figure~\ref{fig:auto-scaler-dist-coord} depicts this architecture.
    \item[Hierarchical Controllers] This architecture is the mixture of previous models. It's a combination of \emph{Global Controller} and \emph{Distributed Coordinated Architecture} model. Local Auto-Scalers communicate with a global controller in order to perform necessary actions. This architecture doesn't impose any limitation on whether local Auto-Scalers communicate with each other or not. Noteworthy to express, one or more back controller might also accompany global controller. Furthermore, it is not necessarily a two-level hierarchy. It might be multi-level hierarchal approach that each level operate in distributed or master-slave mode. Additionally, there might be no limit on who actually performs the actions. Each Auto-Scaler -- whether it is operating in local or global model -- might perform actions independent of the others. Figure~\ref{fig:auto-scaler-hierar} illustrates this architecture.
\end{description}
\begin{figure}[hb]
    \includegraphics[clip, trim=3cm 9cm 2.5cm 2.5cm]{auto-scaler-master-backup.pdf}
    \centering
    \caption{Master-Slave or Global Auto-Scaler Architecture}
    \label{fig:auto-scaler-master-backup}
\end{figure}
\begin{center}
    \begin{figure}
        \includegraphics[clip, trim=3cm 12.2cm 2.5cm 2.5cm]{auto-scaler-full-dist.pdf}
        \centering
        \caption{Distributed Without Coordination Auto-Scaler Architecture}
        \label{fig:auto-scaler-dist-wo-coord}
    \end{figure}
\end{center}
\begin{center}
    \begin{figure}
        \includegraphics[clip, trim=3cm 9cm 2.5cm 2.5cm]{auto-scaler-dist-coord.pdf}
        \centering
        \caption{Distributed Coordinated Auto-Scaler Architecture}
        \label{fig:auto-scaler-dist-coord}
    \end{figure}
\end{center}
\begin{center}
    \begin{figure}
        \includegraphics[clip, trim=3cm 4cm 2.5cm 2.5cm]{auto-scaler-hierarchy.pdf}
        \centering
        \caption{Hierarchical Controllers Auto-Scaler Architecture}
        \label{fig:auto-scaler-hierar}
    \end{figure}
\end{center}

\subsection{Algorithm Family}
\label{ias:alg-fam}

From algorithmic point of view, Auto-Scalers can be classified into 5 different categories.
\begin{itemize}
    \item \textbf{Threshold-Based Policies}
    \item \textbf{Time-Series Analysis}
    \item \textbf{Reinforcement Learning}
    \item \textbf{Queuing Theory}
    \item \textbf{Control Theory}
\end{itemize}
In this section, each category is explained to some detail.

\subsubsection{Threshold-Based Policies}

According to section~\ref{ias:react-proact}, threshold-based approaches follow a purely  reactive approach. It lacks any form of future workload prediction. Threshold-based techniques usually involve a set of constraints which monitors performance data gathered from Metrics Engine. In its naive form, each rule define an \emph{upper} and/or \emph{lower} bound plus two time periods that define how long that specific metric was above the upper threshold or bellow the lower threshold.

Algorithm~\ref{a:threshold-based} better describes this approach in a pseudocode. Refer to section~\ref{related} to review list of proposed solutions.
\begin{algorithm}[h]
    \DontPrintSemicolon
    
    \SetKwFunction{LFC}{LoadFromConfig}
    \SetKwFunction{GCMV}{GetCurrentMetricValue}
    \SetKwFunction{AR}{AcquireResource}
    \SetKwFunction{RR}{ReleaseResource}
    \SetKwFunction{DN}{DoNothingDuringGracePeriod}
    
    \SetKwData{ut}{UpperThreshold}
    \SetKwData{up}{UpperPeriod}
    \SetKwData{lt}{LowerThreshold}
    \SetKwData{lp}{LowerPeriod}
    \SetKwData{x}{currentValue}
    \SetKwData{a}{NumberOfResourcesToAcquire}
    \SetKwData{r}{NumberOfResourcesToRelease}
    
    \tcp{upper threshold}
    \ut $\gets$ \LFC{}\;
    \tcp{time period that performance metric was above upper threshold}
    \up $\gets$ \LFC{}\;
    \tcp{lower threshold}
    \lt $\gets$ \LFC{}\;
    \tcp{time period that performance metric was lower than lower threshold}
    \lp $\gets$ \LFC{}\;
    \tcp{number of resources to acquire in each round}
    \a $\gets$ \LFC{}\;
    \tcp{number of resources to release in each round}
    \r $\gets$ \LFC{}\;
    \BlankLine
    \Repeat{\textup{Auto-Scaler is running}} {
        \x $\gets$ \GCMV{}\;
        \BlankLine
        \If{\x > \ut for \up seconds}{
            \tcp{acquire resource}
            \AR{\a}\;
            \DN{}
        }
        \BlankLine
        \If{\x < \lt for \lp seconds}{
            \tcp{release resource}
            \RR{\r}\;
            \DN{}
        }
    }
    \caption{General work-flow of threshold-based techniques}
    \label{a:threshold-based}
\end{algorithm}

\subsubsection{Time-Series Analysis}

In contrast to threshold-based techniques, time-series analysis approaches are purely proactive or predictive approaches. A \emph{time-series} is a collection of data points sampled and ordered iteratively at uniform time  intervals~\cite{Lorido-Botran2014}. Time-series analysis typically requires to store a history of data points. Hence, storage and computation requirements are more intensive than threshold-based approaches. According to the theory of time-series analysis "TODO reference" time-series can be decomposed into multiple components.
\begin{description}
    \item[Season]
    \item[Trend]
    \item[Noise]
\end{description}


Depending on the algorithm, time-series analysis can absorb up to certain level of noise or outliers.  The history of data points can be modeled in \emph{static} or \emph{sliding} time windows. Then, it can be used to find three types of repetitive workload patterns.

\subsubsection{Reinforcement Learning}

\subsubsection{Queuing Theory}

\subsubsection{Control Theory}

\section{Conclusion}
\label{ias:conc}